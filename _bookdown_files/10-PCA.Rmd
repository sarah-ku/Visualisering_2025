

# Principal component analysis (PCA)

"At lære datalogi er som at lære at stå på ski. Man er nødt til at gøre det." -- Claudia Perlich

```{r,echo=FALSE,comment=FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(broom)
```


## Indledning og læringsmålene


### Læringsmålene

:::goals
Du skal være i stand til at

* Forstå koncepten bag principal component analysis (PCA)
* Benytte PCA i R og lave et plot af et datasæt i to dimensioner
* Vurdere den relative varians forklarede af de forskellige components
* Anvende PCA til at vurdere variablernes bidrag til de principal components
:::

:::checklist
* Se dagens forelæsning om Broom-pakken
* Se videoerne
* Læs 10.4.3 - lidt ekstra tekst om fortolkingen af rotation matrix
* Lav Quiz - Emne 10: PCA
* Lav problemstillingerne
:::

Næste lektion: Workshop 4 om clustering og principal component analyse

### Introduktion til kapitlet

Principal component analysis (PCA) er en meget populær og ofte benyttet statistisk metode indenfor biologiske området. PCA er en metode til dimensionreduktion, der transformerer et datasæt med mange korrelerede variabler til et nyt sæt ortogonale (uafhængige) variable kaldet "principal components" (PC1-PCN, hvor N er antal variabler i datasættet). Det fleste tiden beskæftiger vi os med den første two principle components: 

* PC1 fanger mest mulig varians i datasættet.
* PC2 fanger maksimal varians, som ikke allerede er fanget af PC1, og er orthogonal (vinkelret) på PC1.

Ved at vælge de første  komponenter (ofte 2 eller 3) reducerer vi datasættets dimension, mens vi bevarer størst mulig del af den samlede varians. PCA anvendes bl.a. til:

* Visualisering af højdimensionelle data i 2 eller 3 dimensioner.
* Identifikation af redundante variabler (variabler med høj korrelation).
* Forberedelse til andre metoder, som kræver uafhængige variable.

I biologi bruges PCA typisk til at visualisere kontrol- og behandlingsprøver for at vurdere, om grupperne adskiller sig.

### Videoressourcer

* Video 1 - hvad er PCA?

** Du kan beskrive en dataframe med mange dimensionerne i færre dimensioner.
** Du kan bruge den til at lave et plot med to dimensioner, der indfanger oplysning fra samtlige variabler i dataframen.

```{r,echo=FALSE}
library("vembedr")

#Link her hvis det ikke virker nedenunder: https://player.vimeo.com/video/556581604

embed_url("https://vimeo.com/556581604") #2021
```


---

* Video 2 - hvordan man lave PCA i R og få output i tidy form

```{r,echo=FALSE}

#library("vembedr")

#Link her hvis det ikke virker nedenunder: https://player.vimeo.com/video/556581588


embed_url("https://vimeo.com/556581588") #2021
```

---

* Video 3 - hvordan man visualiserer de data (principal components, rotation matrix)


```{r,echo=FALSE}

#library("vembedr") https://player.vimeo.com/video/556787141

embed_url("https://vimeo.com/556787141") #2021
```

* Efter videoer - lidt ekstra læsning i 10.4.3 (bidragende af de forskellige variabler)

## Hvad er principal component analysis (PCA)?

### Problemet med høj-dimensionelle plots

I sidste lektion arbejdede vi med `penguins`, hvor vi så, at der faktisk var fire numeriske variabler - altså fire dimensioner - som blev brugt til at lave k-means clustering.

```{r}
library(palmerpenguins)
penguins <- penguins %>% 
  drop_na() %>%
  mutate(year=as.factor(year))

penguins %>% select(where(is.numeric)) %>% head()
```

Når man laver et punkt plot for at vise de forskellige clusters, som er beregnet fra alle numeriske variabler, får man et problem - nemlig hvilke to variabler skal plottes? Man kan plotte hver eneste par af variabler. For eksempel kan man prøve en pakke, der hedder `GGally`, som automatisk kan plotte de forskellige par af numeriske variabler og beregner korrelationen mellem variablerne.

```{r}
require(GGally)
penguins %>% 
  ggscatmat(columns = 3:6 ,color = "species", corMethod = "pearson") + 
  scale_color_brewer(palette = "Set2") +
  theme_bw()
```

Men denne løsning bliver hurtigt uoverskuelige for mange variabler.

### Idéen bag PCA: Projektion på nye akser

<!-- En løsning til problemet er at projicere datasættet ned til et mindre antal dimensioner (f.eks. kun 2 dimensioner). Disse dimensioner fanger oplysninger fra alle variablerne i datasættet, og derfor, når man laver et scatter plot, får man repræsenteret det hele datasæt i stedet for kun to udvalgte variabler. Metoden for at lave disse såkaldte 'projektioner' kaldes 'principal component analysis'. -->

Vi ønsker at beregne en ny variabel (PC1), som kan plottes på x-aksen og som bedst fanger den samlede variation i hele datasættet ved at maksimere den projicerede varians. Teknisk set finder man PC1 ved at danne kovarians- eller korrelationsmatrixen og vælge den egenvektor, der svarer til den største egenværdi, fordi netop den retning maksimerer variansen i projektionen. Vi bruger en funktion i R til at udføre PCA, men nogle nøgleord, som er nyttige at huske, er:

* __Centering og scaling:__ PCA kræver typisk, at de numeriske variabler er skaleret.
* __Eigenvalues:__ Fortæller, hvor meget varians hver component fanger.
* __Eigenvectors (loadings):__ Angiver, hvordan de oprindelige variabler kombineres til at danne komponenten.


### Simpelt eksempel med to dimensioner

Man kan forsøge at forstå, hvordan PCA fungerer, ved at kigge på et simpelt eksempel med 2 dimensioner:

```{r,fig.width=4,fig.height=4}
#simulerer data med en høj korrelation
a <- rnorm(250,1,2)
b <- a + rnorm(250,0,.5)
df <- tibble(a,b)
ggplot(df,aes(a,b)) + 
  geom_point() + 
  theme_minimal()
```

Vi kan se her, at der er en meget stor korrelation mellem a og b. Selvom datasættet er plottet i 2 dimensioner, kan det næsten forklares af én linje - en såkaldt bedste rette linje, der passer bedst gennem punkterne.


```{r,fig.width=4,fig.height=4}
df <- tibble(a,b)
ggplot(df,aes(a,b)) + 
  geom_point() + 
  theme_minimal() + 
  geom_smooth(method="lm",se=FALSE)
```


<!-- Med andre ord kan vi næsten forklare datasættet i blot én dimension - punkternes afstand langs linjen. Når man tager alle punkterne og beskriver dem langs én linje, der bedst beskriver variansen i datasættet, kaldes denne linje for den første principal component (PC1). Man kan dernæst beskrive en anden linje, der er vinkelret på PC1, og som bedst forklarer variancen i de data, der ikke blev fanget af PC1 - dette kaldes for den anden principal component (PC2). -->

Lineær regression (`geom_smooth(method="lm")`) illustrerer, at dataene næsten kan beskrives på én linje. Den bedste linje gennem punkterne svarer til PC1, og PC2 findes som en ortogonal linje, der fanger den resterende varians. Vi kan se her PC1 og PC2 plottet:

```{r, echo=FALSE,fig.width = 1,fig.height=1,comment=FALSE,warning=FALSE,out.width="50%"}
# Bigger fig.width
library(png)
library(knitr)
include_graphics("plots/pca_dem.png")
```



Når vi tager PC1 og PC2 og plotter dem som henholdsvis x-aksen og y-aksen, svarer det til en drejning af akserne i plottet (vi finder PC1 og PC2 fra funktionen `prcomp`, som jeg forklarer i næste sektion):

```{r,fig.width=4,fig.height=4}
dat <- augment(prcomp(df),df)
ggplot(dat,aes(x=.fittedPC1,y=.fittedPC2)) + 
  geom_point() + 
  theme_minimal() + 
  geom_smooth(method="lm")
```

Vi kan se her, at dataene fylder pladsen på plottet bedre end før (og bemærk at akseskalaen er blevet meget mindre på den nye y-akse, da dataene spreder sig meget mindre langs PC2 i forhold til PC1.)

Dette er kun et eksempel, hvor vores oprindelige data ligger i to dimensioner (to variabler), for at gøre det nemt at visualisere dem i et plot, men de fleste datasæt (fx penguins, iris osv.) har flere end to dimensioner. Vi kan godt lave den samme proces, hvor vi definerer PC1, som forklarer så meget af variansen i dataene som muligt, og dernæst PC2, som forklarer noget af variansen, der ikke blev fanget af PC1, og dernæst PC3 osv., alt efter hvor mange dimensioner dataene har. I mange praktiske situationer vælger man de første to komponenter, som er de vigtigste, da de forklarer mest af variansen i dataene i forhold til de andre komponenter.


"So to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible." https://builtin.com/data-science/step-step-explanation-principal-component-analysis

## Fit PCA to data in R med `prcomp()`

```{r}
library(broom)
```

Lad os skifte tilbage til nogle virkelige data for at benytte `prcomp`: datasættet `penguins`. Med `prcomp()` fokuserer vi kun på numeriske variabler, så vi bruger `select` med `where(is.numeric)` og anvender derefter skalering ved at specificere `scale = TRUE` inde i funktionen `prcomp`.

<!-- ```{r} -->
<!-- biopsy <- read_csv("https://wilkelab.org/classes/SDS348/data_sets/biopsy.csv") -->
<!-- ``` -->

```{r}
pca_fit <- penguins %>%
  select(where(is.numeric)) %>% # behold kun numeriske kolonner
  prcomp(scale = TRUE) # udfør PCA på skaleret data

summary(pca_fit)
```

`Proportion of Variance` indikerer, hvor meget af variansen i dataene, der blev forklaret af de forskellige komponenter. Vi kan se, at `PC1` forklarer omkring 69\% og de første to komponenter sammen forklarer 88\% af variansen i dataene. Derfor, hvis vi viser et plot af de første to komponenter, ved vi, at vi har fanget rigtig mange oplysninger om de fire variabler i datasættet.

## Integrering af PCA-resultater med broom-pakken

Der er flere ting, som kan være nyttige at gøre med vores PCA-resultater:

* Lave et plot af datasættet ud fra de første to principal components
* Se, hvor meget af variansen i datasættet der er forklaret af de forskellige komponenter
* Bruge rotationsmatricen til at se, hvordan variablerne forholder sig i forhold til hinanden

### Lave plot af principal componenets

For at lave vores plot af principal components kan vi benytte funktionen `augment()`, ligesom vi gjorde i vores sidste lektion med k-means clustering. Her får vi værdierne for hver af de fire principal components sammen med det oprindelige datasæt.

```{r}
pca_fit_augment <- pca_fit %>% 
  augment(penguins) # tilføj det originale datasæt igen

pca_fit_augment
```

Vi kan tage `pca_fit_augment` og lave et plot af de første to principal components:

```{r,fig.width=5,fig.height=4}
pca_fit_augment  %>% 
  ggplot(aes(x=.fittedPC1, y=.fittedPC2, color = species)) + 
  geom_point() +
  theme_bw()
```

Vi kan også integrere de clusters, som vi fik fra funktionen `kmeans()`, i vores PCA ved at anvende funktionen `augment()` på resultaterne fra `kmeans` og vores data, som allerede har resultaterne fra `pca`. Da både PCA og k-means fanger oplysninger om datastrukturen baseret på de fire numeriske variabler, kan man forvente en bedre sammenligning mellem de to (i forhold til at sammenligne clusters med et plot med kun to af variablerne).


```{r,fig.width=5,fig.height=4}
penguins_scaled <- penguins %>% select(where(is.numeric)) %>% scale

kclust <- kmeans(penguins_scaled,centers = 3)

kclust %>% augment(pca_fit_augment)  %>% 
  ggplot(aes(x=.fittedPC1, y=.fittedPC2, color = .cluster)) + 
  geom_point() +
  theme_bw()
```


### Forklarede variance

Næst vil vi se på variansen i datasættet, som er blevet fanget af hver af de forskellige komponenter. Man kan udtrække oplysningerne ved at benytte funktionen `tidy()` fra pakken `broom` og ved at angive `matrix = "eigenvalues"` inden for `tidy`.

Det kaldes "eigenvalues", fordi hvis man kigger på matematikken bag principal component analysis, tager man udgangspunkt i en covariance matrix. En covariance matrix beskriver sammenhængen eller korrelationen mellem de forskellige variabler. Man bruger denne covariance matrix til at beregne de såkaldte eigenvalues og deres tilsvarende eigenvectors.

Det er faktisk den største eigenvalue, der fortæller os om den første principal component - det fortæller os, hvor meget af variansen i datasættet den første principal component fanger - jo større den er i forhold til de andre eigenvalues, jo mere af variansen kan man forklare med den første principal component. Og den næststørste fortæller os om den anden principal component og så videre.

```{r}
pca_fit_tidy <- pca_fit %>%
  tidy(matrix = "eigenvalues")
pca_fit_tidy
```

Lad os visualisere disse tal som procenttal ved at specificere `labels = scales::percent_format()` inden for `scale_y_continuous` - så vi bare ændrer på de tal, der kan ses på y-aksen.

```{r,fig.width=3,fig.height=4}
pca_fit_tidy %>%
  ggplot(aes(x = PC, y = percent)) +
  geom_bar(stat="identity", fill="steelblue") +
  scale_y_continuous(
    labels = scales::percent_format(), #omdann labels til procentformat
  ) +
  theme_minimal()
```

På den ene side, hvis der er meget varians, der er forklaret af de første komponenter tilsammen, betyder det, at der er en del redundans i datasættet, fordi mange af variablerne har en tæt sammenhæng med hinanden. På den anden side, hvis der er en meget lille andel af variansen, der er forklaret af de første komponenter tilsammen, betyder det, at det er svært at beskrive datasættet i færre dimensioner (fordi der næsten ingen sammenhæng er mellem variablerne) - i dette tilfælde, hvor datasættet er mere komplekst, er PCA mindre effektiv.

### Rotationsmatrix: bidragene fra de forskellige variabler

Eigenvalues kan anvendes til at undersøge variansen i datasættet, men deres tilsvarende eigenvectors fortæller os om, hvordan de forskellige variabler kombineres for at opnå de endelige principal component værdier, som vi fx bruger i et scatter plot. Eigenvectors bruges til at lave en matrix, der kaldes en 'rotationsmatrix'. 

Jeg anvender funktionen `pivot_wider` for at gøre vores matrix mere overskuelig at se på. Vi kan se, at vi har variablerne her på rækkerne og de forskellige principal components i kolonnerne.

```{r}
pca_fit_rotate <- pca_fit %>%
  tidy(matrix = "rotation") %>% 
  pivot_wider(names_from = "PC", names_prefix = "PC", values_from = "value")
pca_fit_rotate
```

Denne rotationsmatrix fortæller os, hvordan man beregner værdierne for de principal components for alle observationer. For eksempel tager vi vores første observation, beregner 0.45 gange bill length, og så minus 0.4 gange bill depth, og så plus 0.58 x flipper length og så plus 0.55 x body_mass. Og så har vi værdien for observationen langs den første principal component. 

Man kan lave et plot af hver af de principal components, der viser, hvordan hvert variable bidrage til componentens værdier:

```{r}
pca_fit %>%
  tidy(matrix = "rotation") %>%
  ggplot(aes(value, column,fill=as.factor(PC))) +
  geom_col() +
  facet_wrap(~PC,ncol=1) + 
  theme_bw() + 
  xlab("Contribution to principal component") +
  ylab("Variable") + 
  ggtitle("Rotation matrix values") + 
  scale_fill_brewer(palette="Set1")
```

Her er eksempelvis to fortolkinger ud fra ovenstående plot:

* Pingviner med en større flipper length ville være plottet på højre på den første principal component, men til venstre hvis man plotter din 4. principal componoent.
* Pingviner med en store bill_depth ville være plottet på venstre på både den første principal compenent og den anden principal component.

Vi kan anvende rotationsmatrixen til at se, hvordan de forskellige variabler relaterer til hinanden. Variabler, der er tæt på hinanden i plottet, ligner hinanden. Vi kan se, at `flipper_length_mm` og `body_mass_g` ligner hinanden ret meget i vores datasæt, mens `bill_depth_mm` befinder sig over til venstre langs den første principal component, hvilket indikerer, at den måske indeholder nogle oplysninger om pingvinerne, der ikke kunne fanges i de andre variabler.

```{r}
library(ggrepel)
pca_fit_rotate %>%
  ggplot(aes(x=PC1,y=PC2,colour=column)) + 
  geom_point(size=3) + 
  geom_text_repel(aes(label=column)) + 
  theme_minimal()
```

### Pakken `factoextra`

<!-- To save work making a plot manually, we can just plot this values directly from `pca_fit` using a package `factoextra`: -->

R-pakken `factoextra` kan anvendes til automatisk at lave et lignende plot fra rotationsmatrixen, og den arbejder oven på `ggplot2`, så man kan ændre temaet osv. Du kan se, hvordan det fungerer i følgende kode.

* Man får variansprocenten på akserne.
* Placeringen af pilhovederne kommer fra rotationsmatrixen.
* Jo mindre vinklen mellem to linjer er, og jo tættere de er på hinanden, jo større er sammenhængen mellem de to variable.
* Jo tættere pilhovederne er på cirklen, desto større indflydelse har den pågældende variabel på de principal components.

```{r,comment=FALSE,message=FALSE}
library(factoextra)
fviz_pca_var(pca_fit, col.var="steelblue",repel = TRUE)+
  theme_minimal()
```






## Problemstillinger

__Problem 1__) Quiz på Absalon

---

Vi arbejder med en breast cancer datasæt. Here is the description from Kaggle https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset:

* Breast cancer is the most common cancer amongst women in the world. It accounts for 25% of all cancer cases, and affected over 2.1 Million people in 2015 alone. It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.
* The key challenges against it’s detection is how to classify tumors into malignant (cancerous) or benign(non cancerous)_

Download følgende datasæt ved at køre følgende kode chunk:

```{r}
cancer <- read.csv(url("https://www.dropbox.com/s/4qa37itw9wtwtjg/breast-cancer.csv?dl=1")) %>% as_tibble() %>% select(-id)
cancer %>% glimpse()
```

I variablen `diagnosis`: M betyder 'Malignant' og B betyder 'Benign' - du kan overveje at ændre på variablen i dit indlæste datasæt for at gøre det mere klart. 

---

__Problem 2__)  Først vil vi gerne visualisere datasættet _uden at bruge principal component analyse_. Anvend funktionen `ggscatmat` fra pakken `GGally` til at lave et plot, hvor man sammenligne fem af variablerne. 

* Der er mange variabler, derfor kan man angive en tilfældig sample af fem variabler med `columns = sample(2:31,5)` som parameter indenfor funktionen `ggscatmat`(husk at installere og indlæse `GGally`-pakken).
* Giv farver efter factor variablen `diagnosis` og vælger "pearson" som parameteren `corMethod`.
* Prøv at køre din kode et par gange, så du får forskellige samplings af fem variabler. Opfatter du, at der er en del redundans i datasættet (dvs., er der stærke korrelationer mellem de forskellige variabler, der gør at hvis man ved en værdi fra den ene, kan man gætte på den tilsvarende værdi fra den anden)? 

```{r,echo=FALSE,eval=FALSE}
library(GGally)
cancer %>% 
  ggscatmat(columns = sample(2:31,5) ,color = "diagnosis", corMethod = "pearson") + 
  scale_color_brewer(palette = "Set2") +
  theme_bw()
```

---

__Problem 3__) Benyt funktionen `prcomp()` til at beregne en principal component analysis af datasættet:

* 1. Husk at det skal kun være numeriske variabler og angiv `scale=TRUE` inde i funktionen.
* 2. Anvend `augment()`-funktionen til at tilføje dit rå datasæt til ovenstående resultater fra `prcomp`.
* 3. Brug den til at lave et scatter plot af de første to principal components, hvor du giver farver efter variablen `diagnosis`
* 4. Skriv kort om man kan skelne imellem Malignant og Benign tumours (variablen `diagnosis`) ud fra de første to principal components.
* 5. Skriv også kort - hvilke af de to components er bedre til at skelne mellem Malignant og Benign tumours?


<!-- * Hvad er proportionen af variansen, der er forklaret af den første principal component? -->
<!-- * Hvad er proportionen af variansen, som er forklaret af de første to principal components tilsammen? -->

```{r,echo=FALSE,eval=FALSE}
pca_fit <- cancer %>% select(-diagnosis) %>% prcomp(scale=TRUE)
summary(pca_fit)
```


```{r,echo=FALSE,eval=FALSE}
pca_fit_augment <- pca_fit %>% 
  augment(cancer) # add original dataset back in

pca_fit_augment
```


```{r,echo=FALSE,eval=FALSE}
pca_fit_augment  %>% 
  ggplot(aes(x=.fittedPC1, y=.fittedPC2, color = diagnosis)) + 
  geom_point() +
  theme_bw()
```


<!-- --- -->

<!-- __Problem 4__) *Augment og plot* Anvend `augment()`-funktionen til at tilføje dit rå datasæt til ovenstående resultater fra `prcomp`. -->

<!-- * Brug den til at lave et scatter plot af de første to principal components -->
<!-- * Giv farver efter variablen `diagnosis` -->
<!-- * Skriv kort om man kan skelne imellem Malignant og Benign tumours (variablen `diagnosis`) ud fra de første to principal components. -->
<!-- * Skriv også kort - hvilke af de to components er bedre til at skelne mellem Malignant og Benign tumours? -->

---

__Problem 4__) *Integrere kmeans clustering*. Lav et clustering med `kmeans` på datasættet, med to clusters (husk at udvælge numeriske variabler og scale inden du anvender funktionen `kmeans`), og:

* 1. Augment resultaterne af `kmeans` til dit datasæt, der allerede har `prcomp` resultater tilføjet. 
* 2. Lav et plot og give farver efter `.cluster` og former efter `diagnosis`.
* 3. Sammenligne dine to clusters med `diagnosis`.


```{r,echo=FALSE,eval=FALSE}
cancer_scaled <- cancer %>% select(where(is.numeric)) %>% scale

kclust <- kmeans(cancer_scaled,centers = 2)

kclust_augment <- kclust %>% augment(pca_fit_augment)


kclust_augment  %>% 
  ggplot(aes(x=.fittedPC1, y=.fittedPC2, color = .cluster)) + 
  geom_point(aes(shape=diagnosis)) +
  theme_bw()
```


---


__Problem 5__) *tidy form og variansen* Anvende `tidy(matrix = "eigenvalues")` på din PCA resultater til at få bidragen af de forskellige components til den overordnet varianse i datasættet.

* Lav et barplot som viser de components på x-aksen og `percent` på y-aksen.

```{r,echo=FALSE,eval=FALSE}
pca_fit_tidy <- pca_fit %>%
  tidy(matrix = "eigenvalues")
pca_fit_tidy

pca_fit_tidy %>%
  ggplot(aes(x = PC, y = percent)) +
  geom_bar(stat="identity", fill="steelblue") +
  scale_y_continuous(
    labels = scales::percent_format(), #convert labels to percent format
  ) +
  theme_minimal()
```


---

__Problem 6__) *tidy form og rotation matrix* 

__a__) Anvende `tidy(matrix = "rotation")` til at få den rotation matrix og lav følgende:

* Anvend funktionen `pivot_wider` til at få den til wide form
* Lav et scatter plot som viser bidragerne af de forskellige variabler på den første og den anden principal components
* Anvend `geom_text_repel` til at give labels til de variabler (kan være en god idé at anvend `show.legend=F`)

__b__) Værdierne i den rotation matrix fortæller, hvordan en givet variabel bidrager til den endelige principal component beregning (dvs. værdierne som er plottet i __Problem 4__). Fk. variablen `radius_mean` har en positiv værdi i PC2, som gøre, at en højere værdi af `radius_mean` vil resultatere i en højere værdi på PC2 for en givet observation.

* Kig på placeringen af variablen `compactness_mean` på plottet. Bidrager den negativ eller positiv værdi til PC1? 
* Kig igen på dit plot i __Problem 4__) - hvad effekt ville en forhøjet værdi af `compactness_mean` have på den PC1-værdien til en givet tumour? Ville det gøre det mindre eller mere sandsynligt, at den er "benign"?  

```{r,echo=FALSE,eval=FALSE}
pca_fit_rotate <- pca_fit %>%
  tidy(matrix = "rotation") %>% 
  pivot_wider(names_from = "PC", names_prefix = "PC", values_from = "value")
pca_fit_rotate


library(ggrepel)
pca_fit_rotate %>%
  ggplot(aes(x=PC1,y=PC2,colour=column)) + 
  geom_point(show.legend = F) + 
  geom_text_repel(aes(label=column),show.legend = F) + 
  theme_minimal()
```

---

__Problem 7)__  Udvidelse af __Problem 4)__: Fra din augmented resultater med både dine principal components og clusters: Beregne middelværdierne af din første to principal components for hver af de to clusters. Tilføj dine beregnede middelværdierne til plottet som "x". 

```{r,echo=F,eval=F}
kclust_PCmean <- kclust_augment %>% 
  group_by(.cluster) %>% 
  summarise(".fittedPC1" = mean(.fittedPC1),".fittedPC2" = mean(.fittedPC2))

kclust %>% augment(pca_fit_augment)  %>% 
  ggplot(aes(x=.fittedPC1, y=.fittedPC2)) + 
  geom_point(aes(shape=diagnosis, color = .cluster)) +
  geom_point(data = kclust_PCmean, shape="x",size=10) +
  theme_bw()
```

---

Åbn "world happiness data" og 

```{r}
happiness <- read_csv("https://www.dropbox.com/scl/fi/6rt17anzk31mjyexm16o6/world_happiness_data.csv?rlkey=44qxe2voahqvaxnxlgy01i2ls&dl=1")
happiness %>% glimpse()
```

__Problem 8)__ Lav en principal component analyse på dataframen `happiness`:

* Lav et scatter plot af de first to principal components
* Giv hvert punkt sin egen label efter `Country`

```{r,echo=FALSE}
happy_PCA <- happiness %>%
  select(where(is.numeric)) %>% # behold kun numeriske kolonner
  prcomp(scale = TRUE) # udfør PCA på skaleret data

happy_augment <- happy_PCA %>% augment(happiness) 
```

```{r,echo=FALSE,eval=FALSE}
happy_augment %>% 
  ggplot(aes(.fittedPC1,.fittedPC2)) + 
  geom_point() + 
  geom_text_repel(aes(label=Country))
```

---

__Problem 9)__ Lav barplots der viser bidragene af variablerne til hvert component og angiv en foltolking til en af variablerne.

```{r,echo=FALSE,eval=FALSE}
happy_PCA %>%
  tidy(matrix = "rotation") %>%
  filter(PC<3) %>%
  ggplot(aes(value, column)) +
  geom_col() +
  facet_wrap(~PC)
```

---

__Problem 10)__ Har lande med et "Generosity" af mere end 0.15 signifikant højere værdier på den anden principal component? 

* 1. Opret en kolon, `is_generous` med dine to grupper ("Yes" hvis `Generosity` > 0.15 og "No" hvis ikke). 
* 2. Plotte de "generøs" lande med egen farve på din punkt plot, der viser de første to principal componenter.
* 3. Lav en passende test for at svarer på ovenstående spørgsmålet

```{r,echo=FALSE}
happy_augment <- happy_augment %>% 
mutate(is_generous = ifelse(Generosity>0.15,"Yes","No"))
```


```{r,echo=F,eval=F}
happy_augment %>% 
  ggplot(aes(.fittedPC1,.fittedPC2,colour=is_generous)) + 
  geom_point() + 
  geom_text_repel(aes(label=Country))
```


```{r,echo=FALSE,eval=FALSE}
t1 <- happy_augment %>% dplyr::filter(is_generous=="Yes") %>% pull(.fittedPC1)
t2 <- happy_augment %>% dplyr::filter(is_generous=="No") %>% pull(.fittedPC1)
t.test(t1,t2)
```

---

__Problem 11__ EKSTRA: Kan du lav samme test til de andre principal components (OBS: der er 6 PCs - kan du komme frem til en løsning med funktionel programmering?)

```{r,echo=F,eval=F}
happy_nested <- happy_augment %>% 
pivot_longer(cols=contains(".fitted")) %>% 
group_by(name) %>%
nest() 

#happy_nested %>% pluck("data",1)
mytest <- ~t.test(.x %>% dplyr::filter(is_generous=="Yes") %>% pull(value),.x %>% dplyr::filter(is_generous=="No") %>% pull(value)) %>% glance()

happy_nested %>% mutate(test = map(data,mytest))
```

---

__Problem 12)__ EKSTRA: Gå ind i Kaggle linket til breast cancer dataset (https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset) og klik på "Code". I den "Search" klik på "Filters" til højre og vælge "R" som language. Kig på analyserne, som andre har lavet på samme datasæt.


## Ekstra læsning

Step by step explanation: https://builtin.com/data-science/step-step-explanation-principal-component-analysis

PCA tidyverse style fra claus wilke: https://clauswilke.com/blog/2020/09/07/pca-tidyverse-style/

More PCA in tidyverse framework: https://tbradley1013.github.io/2018/02/01/pca-in-a-tidy-verse-framework/